\documentclass[11pt,twocolumn]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{xcolor}

\usepackage[color=blue]{attachfile2}
\usepackage{cite}
\usepackage{float}

\hypersetup{
	colorlinks=true,
	urlcolor=blue,
	breaklinks=true
}

\usepackage[none]{hyphenat}

\begin{document}

\begin{flushleft}
{\LARGE DLiP -- Assignment 3:\\[1ex]
\textbf{Transfer Learning on Galaxy Zoo 2 Dataset}}
\rule{\linewidth}{0.4pt}
Bradley D.K.C. Spronk\\[1ex]
\today
\end{flushleft}

\section*{Introduction}
Galaxy Zoo 2 is a large-scale dataset of galaxy images labeled by morphological features.
The decision tree for classification is detailed in Willett et al.~\cite{willett2013galaxy}.
Kaggle hosted a competition to predict galaxy morphologies from these images~\cite{kaggle}.

\section*{Methods}

\subsection*{ConvNeXt Model}
I used the small ConvNeXt model\cite{convnext}, a modern convolutional neural network with a feature extractor, fully connected layers, and an output layer (see Fig.~\ref{fig:ConvNeXt-structure}).
Each block consists of depthwise convolution, layer normalization, pointwise convolution, and GELU activation.
The Galaxy Zoo 2 dataset was preprocessed by cropping, resizing, together with ConvNeXt-specific mean and standard deviation.
Training was performed in two ways: on the full dataset (normal training) and with curriculum learning.
In both cases, the last two feature stages and the classifier were fine-tuned.
The optimizer used separate learning rates for backbone ($1\mathrm{e-}4$) and classifier ($5\mathrm{e-}4$), and a weight decay of $1\mathrm{e-}4$.
The model was trained for 20 epochs with a batch size of 512.

\subsection*{Transfer Learning}
Transfer learning is a technique where weights from a model trained on a large dataset are reused for a related task~\cite{transfer_learning}.
I fine-tuned a pre-trained ConvNeXt model on Galaxy Zoo 2 by updating the last two feature stages and the classifier, efficiently adapting it to the new task.

\subsection*{Curriculum Learning}
Curriculum learning is a strategy where a model is first trained on easier examples and gradually exposed to harder ones.\cite{curriculum}
For this project, I split the training data into three equal stages based on label confidence: high (\(>0.94\)), medium (0.85-0.94), and low (\(<0.85\)).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{ConvNeXt-structure.png}
    \caption{ConvNeXt architecture\cite{geeksforgeeks}: convolutional feature extractor, fully connected, and output layers. The last two feature stages and classifier are fine-tuned.}
    \label{fig:ConvNeXt-structure}
\end{figure}

\section*{Results}
Figure~\ref{fig:results_convnext_last_layer} presents the training loss of the ConvNeXt model, with the results for standard training depicted in gray and those for curriculum learning illustrated in shades of green and blue.
The results indicate that curriculum learning can improve classification performance by allowing the model to learn from easier examples before tackling more difficult ones.

The normal and curriculum learning models achieved final Kaggle RMSE scores of 0.XXXXX and 0.XXXXX, respectively.
For comparison, the competition winner scored 0.07491.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{result_both.png}
    \caption{Training loss of the ConvNeXt model with and without curriculum learning. Note that this is not RMSE.}
    \label{fig:results_convnext_last_layer}
\end{figure}

\section*{Discussion}
My training was limited by hardware constraints, with each epoch requiring 6 minutes.
Extending training with at least 10 more epochs or fine-tuning additional layers will improve the performance of both normal and curriculum learning.
Curriculum learning's initial loss is higher due to a smaller dataset.
The model quickly learns to classify easier examples, with loss decreasing significantly faster than normal training.
One thing to note is that while it seems that both models arrive at a similar final loss in the same number of epochs, the curriculum learning model took less (real) time to get there.
Curriculum learning was around 17\% faster than normal learning, total training time was 1.5 hours and 1.25 hours for normal learning and curriculum learning respectively.
It is unclear if this behavior will persist with different hyperparameters.

\section*{Conclusion}
Transfer learning with ConvNeXt enables decent galaxy morphology classification on Galaxy Zoo 2.
Further improvements could be achieved by increasing training duration, or fine-tuning more layers.
Or, of course, using a bigger model and retraining it completely.
Curriculum learning shows promise in reducing training time, however further investigation is needed.
My results show that data selection strategies also impact results, with direct transition from high-confidence to full dataset showing promise.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
