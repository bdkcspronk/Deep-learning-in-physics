\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage[color=blue]{attachfile2}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{pdfpages}

\hypersetup{
	colorlinks=true,
	urlcolor=blue,
	breaklinks=true
}

\title{DLIP - Assignment 1: Training}
\author{Bradley Spronk - s2504758}
\date{\today}

\begin{document}
\maketitle

\section{Python data science refresher.}
\href{https://github.com/bdkcspronk/Computational-Physics/tree/main}{Link of github repository to recent python projects}. These python projects wer based on two assignments for the course Computational Physics. In these projects I used packages such as pickle, numpy, numba, and matplotlib for data handling, efficient computation, and visualization. And they show my python skills in data science, including data manipulation, debugging, and visualization.

\section{Tensorflow playground.}

\subsection*{Duarte Homework 1 Problem 4A}

\subsubsection*{Linear model}

It is not possible to fit the data with a linear model with only the features \(x1\) and \(x2\), because the data is not linearly separable.
Or in other words, we cannot draw a straight line that separates the two classes of points in the feature space defined by \(x1\) and \(x2\).

However, if we add the feature \(x1x2\), we can fit the data perfectly with a linear model. Because it allows us to capture the interaction between \(x1\) and \(x2\), which is exactly what is necessary to separate the classes. The feature \(x1x2\) creates a new dimension in the feature space that allows us to draw a linear decision boundary that separates the two classes of points.
\cite{kernel_trick}

\subsubsection*{RelU activation model}
With ReLU, a neural network with 1 hidden layer with 4 neurons is the smallest neural network that fits the training data perfectly (with a training loss of \(\leq 0.001\)). The test loss is 0.001 and could go to 0 if we increase the number of epochs however in general we then risk overfitting. The configuration can be seen in Fig.\ref{fig:playground1}. In theory it should be possible to fit the data with a smaller network, but in practice it is not possible to find the right weights and biases to achieve that.\cite{xor_playwithml} Additionally, the ReLU activation function can lead to dead neurons (neurons that output 0 for all inputs), which can further complicate the training process for smaller networks.

\url{https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.14959&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}

\textit{Note: We can't change the output neurons activation. For instance when there is no hidden layer and we choose the ReLU model (\(\max(0,x)\)) the output still shows negative numbers. Possibly a sigmoid function as the last neuron is being used to convert the output to a probability, but this is not explicitly stated in the playground.}


\subsection*{Duarte Homework 1: Problem 4B}

\subsubsection*{Spiral dataset with all features}
A neural network with 1 hidden layer with 6 neurons can fit the spiral dataset with all available features. This configuration can be seen in Fig.\ref{fig:playground_spiral}. The training error is 0.000 and the test error is 0.010. This is a low bias/low variance model, because it fits the training data perfectly (low bias) and also generalizes well to the test data (low variance). However, as can be seen from the figure the decision boundary does not separate the classes in an intelligent way, which suggests that the model is not learning a meaningful representation of the data and is likely overfitting to the training data. Possible features to add to reduce the variance and improve the generalization of the model could be adding the features \(\sqrt(x1^2+x2^2)\) or \(\arctan(x2/x1)\) to capture the radial and angular structure of the spiral dataset.

\url{https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.78711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false}

\subsubsection*{Spiral dataset with only \(x1\) and \(x2\) features}

A neural network with 3 hidden layers with 8, 7, and 6 neurons respectively can fit the spiral dataset with only the features \(x1\) and \(x2\). This configuration can be seen in Fig.\ref{fig:playground_spiral_x1_x2}. The absence of additional features means that the neural network has to rely solely on the raw input features \(x1\) and \(x2\) to learn the decision boundaries, which can make it more challenging to fit the data and may require a larger network to achieve good performance. A spiral pattern is simply not linearly separable in the original feature space, so the network needs to have a more hidden layers to separate the classes than in the previous case. The test error is 0.036, which is even worse than before, and the decision boundary is even less intelligent with gaps in between the spiral arms and rays shooting out, which suggests that the model is likely overfitting to the training data and may not generalize well to unseen data. Large spikes in training/test error on the spiral dataset typically indicate unstable optimization, probaly due to an aggressive learning rate.

\url{https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,7,6&seed=0.20569&showTestData=false&discretize=true&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false}

\clearpage
\begin{figure}[p]
	\centering
	\includegraphics[width=0.8\textwidth]{playground.png}
	\caption{Screenshot\cite{playground} of the smallest neural network that fits the training data perfectly.}
	\label{fig:playground1}

	\includegraphics[width=0.8\textwidth]{playground_spiral.png}
	\caption{Screenshot\cite{playground} of a neural network that can fit a spiral dataset with all available features.}
	\label{fig:playground_spiral}

    \includegraphics[width=0.8\textwidth]{playground_spiral_x1_x2.png}
	\caption{Screenshot\cite{playground} of a neural network that can fit a spiral dataset with only the features \(x1\) and \(x2\).}
	\label{fig:playground_spiral_x1_x2}
\end{figure}

\clearpage
\section{Training curves for polynomial regression.}

\subsection*{Brightspace question about training error trends with epochs and data size}

With a small dataset the model can effectively memorize the training points, but as more data is added it must generalize across more variation, which can raise the training error while it learns a more faithful model of the data.

\subsection*{Duarte Homework 1 Problem 2}

\subsubsection*{Problem 2 Part 1A}

We can make our algebra and coding simpler by writing $f (x_1, x_2, \ldots, x_d) = \mathbf{w}^\top \mathbf{x}$
for vectors $\mathbf{w}$ and $\mathbf{x}$. But at first glance, this formulation seems to be missing the bias term $b$ from
the equation above. How should we define $\mathbf{x}$ and $\mathbf{w}$ such that the model includes the bias term?

\textit{Hint: Include an additional element in $\mathbf{w}$ and $\mathbf{x}$}

\textbf{Solution:}

We can define vectors $ \mathbf{x} $ and $ \mathbf{w} $ as:

\begin{equation}
\mathbf{x} = [x_1, x_2, \ldots, x_d, 1], \quad
\mathbf{w} = [w_1, w_2, \ldots, w_d, \mathrm{bias}]
\end{equation}

So that the dot product gives:

\begin{equation}
\mathbf{w}^\top \mathbf{x} = w_1 x_1 + w_2 x_2 + \ldots + w_d x_d + 1 \cdot \mathrm{bias}
\end{equation}

\subsubsection*{Problem 2 Part 1B: Gradient of the Squared Loss}

For a single data point $(\mathbf{x}, y)$, the squared loss is:

\begin{equation}
L(\mathbf{w}) = (y - \mathbf{w}^\top \mathbf{x})^2
\end{equation}

The gradient of $L$ with respect to $\mathbf{w}$ is obtained using the chain rule:

\begin{align}
\nabla_{\mathbf{w}} L(\mathbf{w}) 
&= \frac{\partial}{\partial \mathbf{w}} (y - \mathbf{w}^\top \mathbf{x})^2 \\
&= 2 (y - \mathbf{w}^\top \mathbf{x}) \cdot \frac{\partial}{\partial \mathbf{w}} (y - \mathbf{w}^\top \mathbf{x}) \\
&= 2 (y - \mathbf{w}^\top \mathbf{x}) \cdot (-\mathbf{x}) \\
&= -2 (y - \mathbf{w}^\top \mathbf{x}) \mathbf{x}
\end{align}

\includepdf[pages=-]{code/Homework1-problem2.pdf}

\includepdf[pages=-]{code/Homework2-problem1.pdf}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
